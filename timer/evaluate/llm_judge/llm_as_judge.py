
import pandas as pd
import json
import os
import sys
import requests
import time
from typing import Dict, Any, List
import tiktoken
import xml.etree.ElementTree as ET
from typing import List
import math
import signal
import argparse

# Azure OpenAI settings
OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
DEPLOYMENT_NAME = "gpt-4o-mini"
API_VERSION = "2023-05-15"
print(f"OPENAI_API_BASE: {OPENAI_API_BASE}")
print(f"OPENAI_API_KEY: {OPENAI_API_KEY}")


def log_to_file(file_path: str, message: str):
    with open(file_path, 'a') as f:
        f.write(message + '\n')

def get_tokenizer():
    return tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    tokenizer = get_tokenizer()
    return len(tokenizer.encode(text))

def trim_xml_tree(root: ET.Element, max_tokens: int) -> ET.Element:
    tokenizer = get_tokenizer()
    new_root = ET.Element(root.tag, root.attrib)
    current_tokens = 0

    for child in reversed(root):
        child_xml = ET.tostring(child, encoding='unicode')
        child_tokens = len(tokenizer.encode(child_xml))
        
        if current_tokens + child_tokens <= max_tokens:
            new_root.insert(0, child)
            current_tokens += child_tokens
        else:
            break

    return new_root

def trim_ehr(ehr_data: str, max_tokens: int) -> str:
    root = ET.fromstring(ehr_data)
    trimmed_root = trim_xml_tree(root, max_tokens)
    return ET.tostring(trimmed_root, encoding='unicode')

def completion_with_backoff(**kwargs) -> Dict[str, Any]:
    retry_count = 0
    while True:
        retry_count += 1
        try:
            url = f"{OPENAI_API_BASE}/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}"
            headers = {
                "Ocp-Apim-Subscription-Key": OPENAI_API_KEY,
                "Content-Type": 'application/json'
            }
            data = {
                "messages": kwargs['messages'],
                "max_tokens": kwargs.get('max_tokens', 1000),
                "temperature": kwargs.get('temperature', 0.7)
            }
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as error:
            print(f"Error: {error}")
            if retry_count > 30:
                return {}
            time.sleep(10)

def create_prompt(ehr_data: str, instruction: str, model_response: str, reference_answer: str) -> str:
    return f"""You are an expert in electronic health records (EHR) analysis. Your task is to evaluate responses generated by AI models based on given instructions and EHR. You will assess the quality of the responses based on specific criteria, comparing them to provided reference answers (ground truth). Aim to be fair and balanced in your evaluation, recognizing both strengths and limitations in the model's response.

EHR Data:
{ehr_data}

Instruction:
{instruction}

Model Response:
{model_response}

Reference Answer:
{reference_answer}

Evaluation Criteria:
1. Correctness (0/1):
   - Score 1 if the response is generally accurate and aligns with the key points in the reference answer, even if there are minor discrepancies or omissions.
   - Score 0 only if the response contains significant factual errors or clearly misinterprets the EHR data.

2. Completeness (0/1):
   - Score 1 if the response addresses the main aspects of the given instruction and covers the essential points present in the reference answer.
   - Score 0 only if the response misses critical information or fails to address the core of the instruction.

Remember, the goal is to evaluate the overall quality and usefulness of the response. Minor imperfections should not necessarily result in a score of 0 if the response is generally correct and complete.

Please provide your evaluation in the following JSON format:
{{
  "evaluation": {{
    "correctness": {{
      "score": 0 or 1,
      "explanation": "Your reasoning here, including comparison to reference answer"
    }},
    "completeness": {{
      "score": 0 or 1,
      "explanation": "Your reasoning here, including comparison to reference answer"
    }}
  }},
  "overall_comments": "Brief summary comparing the model's response to the reference answer, highlighting strengths and areas for improvement"
}}

Ensure your output is valid JSON that can be parsed programmatically. Do not include any text outside of the JSON structure.
"""

def evaluate_response(ehr_data: str, instruction: str, model_response: str, reference_answer: str, context_length:int) -> Dict[str, Any]:
    TOTAL_CONTEXT_LENGTH = context_length
    prompt=create_prompt("", "", "", "")
    EVALUATION_PROMPT_TOKENS = count_tokens(prompt)  
    
    for name, value in [("ehr_data", ehr_data), ("instruction", instruction), 
                        ("model_response", model_response), ("reference_answer", reference_answer)]:
        if value is None or (isinstance(value, float) and math.isnan(value)):
            print(f"Warning: {name} is None or NaN")
            value = "[No response provided]"
        elif not isinstance(value, str):
            print(f"Warning: {name} is not a string. Attempting to convert.")
            try:
                value = str(value)
            except Exception as e:
                print(f"Error converting {name} to string: {e}")
                value = "[Error: Unable to process response]"
        locals()[name] = value

    # Calculate available tokens for EHR
    try:
        instruction_tokens = count_tokens(instruction)
        model_response_tokens = count_tokens(model_response)
        reference_answer_tokens = count_tokens(reference_answer)
    except Exception as e:
        print(f"Error counting tokens: {e}")
        return None
    
    available_ehr_tokens = TOTAL_CONTEXT_LENGTH - (
        EVALUATION_PROMPT_TOKENS + 
        instruction_tokens + 
        model_response_tokens + 
        reference_answer_tokens
    )
    
    available_ehr_tokens = max(available_ehr_tokens, 100)

    # Trim the EHR data
    trimmed_ehr = trim_ehr(ehr_data, available_ehr_tokens)
    
    prompt = create_prompt(trimmed_ehr, instruction, model_response, reference_answer)
    
    response = completion_with_backoff(
        messages=[
            {"role": "system", "content": "You are an AI assistant tasked with evaluating EHR analysis responses."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=2000,
        temperature=0.2,
    )
    
    try:
        evaluation = json.loads(response['choices'][0]['message']['content'])
        return evaluation
    except (json.JSONDecodeError, KeyError, IndexError) as e:
        print(f"Error parsing LLM response: {e}")
        print("Raw response:")
        print(response)
        return None

def load_ehr_data(filename: str, ehr_directory: str) -> str:
    file_path = os.path.join(ehr_directory, filename)
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        return ET.tostring(root, encoding='unicode', method='xml')
    except ET.ParseError as e:
        print(f"Error parsing XML file {file_path}: {e}")
        return ""

def safe_get(dictionary, *keys):
    """Safely get a value from a nested dictionary."""
    for key in keys:
        try:
            dictionary = dictionary[key]
        except (KeyError, TypeError):
            return None
    return dictionary

def extract_score(value):
    """Extract a numeric score from a value, handling various formats."""
    if isinstance(value, (int, float)):
        return value
    elif isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            try:
                return float(value)
            except ValueError:
                return None
    return None

def format_evaluation(evaluation):
    """Format the evaluation data in a robust manner and extract scores."""
    formatted = {
        "correctness": {
            "score": safe_get(evaluation, "evaluation", "correctness", "score"),
            "explanation": safe_get(evaluation, "evaluation", "correctness", "explanation")
        },
        "completeness": {
            "score": safe_get(evaluation, "evaluation", "completeness", "score"),
            "explanation": safe_get(evaluation, "evaluation", "completeness", "explanation")
        },
        "overall_comments": safe_get(evaluation, "overall_comments")
    }
    
    # If the expected structure is not found, try to extract scores from the raw evaluation
    if all(v is None for v in [formatted['correctness']['score'], formatted['completeness']['score']]):
        formatted['raw_evaluation'] = evaluation
        for key, value in evaluation.items():
            if 'correct' in key.lower():
                formatted['correctness']['score'] = extract_score(value)
            elif 'complete' in key.lower():
                formatted['completeness']['score'] = extract_score(value)
    
    return formatted

def calculate_average_scores(results: List[Dict[str, Any]]) -> Dict[str, float]:
    total_correctness = 0
    total_completeness = 0
    count_correctness = 0
    count_completeness = 0

    for result in results:
        evaluation = result.get('evaluation', {})
        correctness_score = extract_score(safe_get(evaluation, 'correctness', 'score'))
        completeness_score = extract_score(safe_get(evaluation, 'completeness', 'score'))
        
        if correctness_score is not None:
            total_correctness += correctness_score
            count_correctness += 1
        if completeness_score is not None:
            total_completeness += completeness_score
            count_completeness += 1

    return {
        "correctness": total_correctness / count_correctness if count_correctness > 0 else 0,
        "completeness": total_completeness / count_completeness if count_completeness > 0 else 0
    }

def timeout_handler(signum, frame):
    raise TimeoutError("Evaluation timed out")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_generated_responses', required=True, type=str)
    parser.add_argument('--output_file', required=True, type=str)
    parser.add_argument('--ehr_dir', type=str, default="../data/MedAlign/full_patient_ehrs")
    parser.add_argument('--reference_answers', required=True, type=str)
    parser.add_argument('--context_length', required=True, type=int)

    args = parser.parse_args()

    MODEL_OUTPUT_FILE = args.model_generated_responses
    OUTPUT_JSON_FILE = args.output_file
    REFERENCE_FILE= args.reference_answers
    EHR_DIRECTORY = args.ehr_dir
    CONTEXT_LENGTH = args.context_length

    reference_data = pd.read_csv(REFERENCE_FILE)
    model_output_data = pd.read_csv(MODEL_OUTPUT_FILE)
    reference_data_annotator1 = reference_data[reference_data['annotator_num'] == 'Annotator_1']
    model_output_dict = dict(zip(model_output_data['instruction_id'], model_output_data['model_response']))
    ehr_cache = {}

    if os.path.exists(OUTPUT_JSON_FILE):
        with open(OUTPUT_JSON_FILE, 'r') as f:
            existing_results = json.load(f)
        results = existing_results.get('detailed_results', [])
        processed_ids = set(result['instruction_id'] for result in results)
    else:
        results = []
        processed_ids = set()

    signal.signal(signal.SIGALRM, timeout_handler)

    for _, row in reference_data_annotator1.iterrows():
        instruction_id = row['instruction_id']
        
        if instruction_id in processed_ids:
            print(f"Instruction {instruction_id} already processed. Skipping.")
            continue

        try:
            signal.alarm(300)  

            if instruction_id in model_output_dict:
                ehr_data = ehr_cache.get(row['filename']) or load_ehr_data(row['filename'], EHR_DIRECTORY)
                ehr_cache[row['filename']] = ehr_data
                
                model_response = model_output_dict[instruction_id]
                if isinstance(model_response, float) and math.isnan(model_response):
                    model_response = "[No response provided]"

                evaluation = evaluate_response(ehr_data, row['question'], model_response, row['clinician_response'], CONTEXT_LENGTH)
                
                if evaluation:
                    results.append({
                        "instruction_id": instruction_id,
                        "ehr_filename": row['filename'],
                        "instruction": row['question'],
                        "reference_answer": row['clinician_response'],
                        "model_response": model_response,
                        "evaluation": format_evaluation(evaluation)
                    })
                    print(f"Evaluated instruction {instruction_id}")

                    with open(OUTPUT_JSON_FILE, "w") as f:
                        json.dump({
                            "average_scores": calculate_average_scores(results),
                            "detailed_results": results
                        }, f, indent=2)

            signal.alarm(0) 

        except TimeoutError:
            print(f"Evaluation timed out for instruction {instruction_id}")
        except Exception as e:
            print(f"Error processing instruction {instruction_id}: {str(e)}")

    average_scores = calculate_average_scores(results)
    print(f"Evaluation complete. Results saved to {OUTPUT_JSON_FILE}")
    print(f"Average Correctness Score: {average_scores['correctness']:.3f}")
    print(f"Average Completeness Score: {average_scores['completeness']:.3f}")

if __name__ == "__main__":
    main()
